{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUCOU\n"
     ]
    }
   ],
   "source": [
    "print(\"COUCOU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pycuda.compiler as cuda_compiler\n",
    "from pycuda.gpuarray import GPUArray\n",
    "import pycuda.driver as cuda_driver\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import IPythonMagic\n",
    "from Timer import Timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python version 3.6.6 | packaged by conda-forge | (default, Jul 26 2018, 09:53:17) \n",
      "[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]\n"
     ]
    }
   ],
   "source": [
    "%setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering context in user workspace\n",
      "Creating context\n",
      "PyCUDA version 2018.1.1\n",
      "CUDA version (8, 0, 0)\n",
      "Driver version 9010\n",
      "Using 'Quadro K2000' GPU\n",
      " => compute capability: (3, 0)\n",
      " => memory: 1799 / 1999 MB available\n",
      "Created context handle <29454528>\n",
      "Using CUDA cache dir /home/jobic/test/cuda/milan2018/MilanoGPU2018/notebooks/cuda_cache\n"
     ]
    }
   ],
   "source": [
    "%cuda_context_handler context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jobic/anaconda3/envs/gpudev/lib/python3.6/site-packages/ipykernel/__main__.py:61: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu(6): warning: variable \"gid\" was declared but never referenced\n",
      "\n",
      "kernel.cu(6): warning: variable \"gid\" was declared but never referenced\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kernel_src=\"\"\"\n",
    "__global__ void shmemReduction(float* output, float *input, int size) {\n",
    "    //First we stride through global memory and compute\n",
    "    //the maximum for every thread\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x; //threadIdx.x == 0 because we use just one block\n",
    "    \n",
    "    float max_value = -9999999; //FIXME : Use proprer value here\n",
    "    for (int i = threadIdx.x; i < size; i += blockDim.x) {\n",
    "        max_value = fmaxf(max_value,input[i]);\n",
    "    }\n",
    "    \n",
    "    //Temporary write to memory to check if things work so far\n",
    "    output[threadIdx.x] = max_value;\n",
    "    \n",
    "    //Store the per-thread maximum in shared memory\n",
    "    __shared__ float max_shared[128];\n",
    "    max_shared[threadIdx.x] = max_value;\n",
    "    \n",
    "    //synchronize so that all thread see the same shared memory\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Find the maximum in shared memory\n",
    "    // Reduce from 128 to 64 elements\n",
    "    if (threadIdx.x < 64) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x],max_shared[threadIdx.x + 64]);\n",
    "    }\n",
    "    //Since we here have monre than one active wrap (threadIdx.x>64)\n",
    "    //we want to make sure everyone finished before continuing\n",
    "    __syncthreads();    \n",
    "    \n",
    "    // Reduce from 64 to 32 elements\n",
    "    if (threadIdx.x < 32) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x],max_shared[threadIdx.x + 32]);\n",
    "    }\n",
    "    if (threadIdx.x < 16) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x],max_shared[threadIdx.x + 16]);\n",
    "    }\n",
    "    // Reduce from 16 to 8 elements\n",
    "    if (threadIdx.x < 8) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x],max_shared[threadIdx.x + 8]);\n",
    "    }\n",
    "    // Reduce from 8 to 4 elements\n",
    "    if (threadIdx.x < 4) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x],max_shared[threadIdx.x + 4]);\n",
    "    }\n",
    "    // Reduce from 4 to 2 elements\n",
    "    if (threadIdx.x < 2) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x],max_shared[threadIdx.x + 2]);\n",
    "    }    \n",
    "    // Reduce from 2 to 1 elements\n",
    "    if (threadIdx.x < 1) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x],max_shared[threadIdx.x + 1]);\n",
    "    }    \n",
    "    \n",
    "    //finally write out the output\n",
    "    if (threadIdx.x == 0) {\n",
    "        output[0] = max_shared[0];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "kernel_module = cuda_compiler.SourceModule(kernel_src)\n",
    "kernel_function = kernel_module.get_function(\"shmemReduction\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 128\n",
    "a = np.random.random((1,n)).astype(np.float32)\n",
    "\n",
    "a_g = GPUArray(a.shape, a.dtype)\n",
    "a_g.set(a)\n",
    "\n",
    "num_threads = 16\n",
    "b = np.empty((1,num_threads)).astype(np.float32)\n",
    "b_g = GPUArray(b.shape, b.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.16531369 0.10954849 0.43433616 0.9187092  0.6890633\n",
      "  0.8207824  0.95822036 0.03150816 0.12904522 0.616344   0.42157936\n",
      "  0.16975747 0.8022929  0.19909826 0.47214758]]\n",
      "0.9879879\n"
     ]
    }
   ],
   "source": [
    "grid_size = (num_threads, 1, 1)\n",
    "block_size = (1, 1, 1)\n",
    "\n",
    "kernel_function(b_g,a_g,np.int32(n),grid=grid_size,block=block_size)\n",
    "\n",
    "b_g.get(b)\n",
    "\n",
    "#print(a)\n",
    "print(b)\n",
    "print(np.max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpudev]",
   "language": "python",
   "name": "conda-env-gpudev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
