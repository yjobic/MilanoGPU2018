{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "    IPython notebook for running shared memory reductions\n",
    "    Copyright (C) 2018 Andre.Brodtkorb@ifi.uio.no\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "import pycuda.driver as cuda_driver\n",
    "import pycuda.compiler as cuda_compiler\n",
    "from pycuda.gpuarray import GPUArray\n",
    "\n",
    "import IPythonMagic\n",
    "from Timer import Timer\n",
    "\n",
    "import pytest\n",
    "from ipytest import run_pytest, clean_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global logger already initialized!\n"
     ]
    }
   ],
   "source": [
    "%setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering context in user workspace\n",
      "Context already registered! Ignoring\n"
     ]
    }
   ],
   "source": [
    "%cuda_context_handler context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_src = \"\"\"\n",
    "\n",
    "#include <float.h>\n",
    "\n",
    "__global__ void shmemReduction(float* output, float* input, int size) {\n",
    "    // First we stride through global mememory and compute\n",
    "    // the maximum for every thread\n",
    "    float max_value = -FLT_MAX; //FIXME: Use proper value here\n",
    "    for (int i = threadIdx.x; i < size; i = i + blockDim.x) {\n",
    "        max_value = fmaxf(max_value, input[i]);\n",
    "    }\n",
    "    \n",
    "    // Store the per-thread maximum in shared memory\n",
    "    __shared__ float max_shared[512];\n",
    "    max_shared[threadIdx.x] = max_value;\n",
    "    \n",
    "    // Synchronze so that all thread see the same shared memory\n",
    "    __syncthreads();\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    // Find the maximum in shared memory\n",
    "    //Reduce from 512 to 256 elements\n",
    "    if (threadIdx.x < 256) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 256]);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    //Reduce from 256 to 128 elements\n",
    "    if (threadIdx.x < 128) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 128]);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    //Reduce from 128 to 64 elements\n",
    "    if (threadIdx.x < 64) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 64]);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    //Reduce from 32 to 16 elements\n",
    "    //Since we here have only one active warp (threadIdx.x > 32)\n",
    "    //we do not need to call syncthreads anymore\n",
    "    volatile float* p = &max_shared[0]; //To help the compiler not cache this variable...\n",
    "    if (threadIdx.x < 32) {\n",
    "        p[threadIdx.x] = fmaxf(p[threadIdx.x], p[threadIdx.x + 32]);\n",
    "        p[threadIdx.x] = fmaxf(p[threadIdx.x], p[threadIdx.x + 16]);\n",
    "        p[threadIdx.x] = fmaxf(p[threadIdx.x], p[threadIdx.x + 8]);\n",
    "        p[threadIdx.x] = fmaxf(p[threadIdx.x], p[threadIdx.x + 4]);\n",
    "        p[threadIdx.x] = fmaxf(p[threadIdx.x], p[threadIdx.x + 2]);\n",
    "        p[threadIdx.x] = fmaxf(p[threadIdx.x], p[threadIdx.x + 1]);\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    // Finally write out to output\n",
    "    if (threadIdx.x == 0) {\n",
    "        output[0] = p[0];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "kernel_module = cuda_compiler.SourceModule(kernel_src)\n",
    "kernel_function = kernel_module.get_function(\"shmemReduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMaxGPU(a):\n",
    "    a_g = GPUArray(a.shape, a.dtype)\n",
    "    a_g.set(a)\n",
    "\n",
    "    b = np.empty((1, 1), dtype=np.float32)\n",
    "    b_g = GPUArray(b.shape, b.dtype)\n",
    "    \n",
    "    num_threads = 512\n",
    "    block_size = (num_threads, 1, 1)\n",
    "    grid_size = (1, 1, 1)\n",
    "\n",
    "    kernel_function(b_g, a_g, np.int32(a.shape[1]), grid=grid_size, block=block_size)\n",
    "\n",
    "    b_g.get(b)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.6.6, pytest-3.8.2, py-1.6.0, pluggy-0.7.1 -- /usr/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/ubuntu/jupyter_notebooks/Andre Brodtkorb/MilanoGPU2018/notebooks, inifile:\n",
      "collecting ... collected 1 item\n",
      "\n",
      "13 Shared memory.py::test_findMaxGPU <- <ipython-input-19-e561f4232239> PASSED [100%]\n",
      "\n",
      "=========================== 1 passed in 0.28 seconds ===========================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tests()\n",
    "\n",
    "def test_findMaxGPU():\n",
    "    #Run through each element and check that we can find it\n",
    "    n = 1024\n",
    "    a = np.ones((1, n), dtype=np.float32)\n",
    "    for i in range(n):\n",
    "        a[0,i] = 2.0\n",
    "\n",
    "        b = findMaxGPU(a)\n",
    "        \n",
    "        a[0,i] = 1.0\n",
    "\n",
    "        assert b == 2.0\n",
    "        \n",
    "    #Test a random dataset\n",
    "    a = np.random.random((1, n)).astype(np.float32)\n",
    "    b = findMaxGPU(a)\n",
    "    assert np.max(a) == b\n",
    "        \n",
    "run_pytest(filename='13 Shared memory.ipynb', pytest_options=['-vvv'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
